{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingWindow(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BPM = 30\n",
    "MIN_FREQ = 300  # from inspection of '.hea' content\n",
    "SAMPLE_WINDOW = int(60 / MIN_BPM * MIN_FREQ)\n",
    "FIRST_WINDOW = int(SAMPLE_WINDOW / 2)  # avoid artifacts in the beginning of signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t17 = pd.read_csv('./data/training2017/REFERENCE.csv', names=['id', 'type'])\n",
    "t17['fname'] = './data/training2017/' + t17.id\n",
    "t17['noisy'] = t17.type == '~'\n",
    "\n",
    "saa = pd.read_csv('./data/set-a/RECORDS-acceptable', names=['id'])\n",
    "saa['fname'] = './data/set-a/' + saa.id.astype(str)\n",
    "saa['noisy'] = False\n",
    "sau = pd.read_csv('./data/set-a/RECORDS-unacceptable', names=['id'])\n",
    "sau['fname'] = './data/set-a/' + sau.id.astype(str)\n",
    "sau['noisy'] = True\n",
    "sa = pd.concat([saa, sau])\n",
    "\n",
    "val = pd.read_csv('./data/validation/REFERENCE.csv', names=['id', 'type'])\n",
    "val['fname'] = './data/validation/' + val.id\n",
    "val['noisy'] = val.type == '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_noisy = []\n",
    "train_validate_clean = []\n",
    "\n",
    "for idx, row in t17.iterrows():\n",
    "    data, info = wfdb.rdsamp(row['fname'])\n",
    "    for sgnl in data.T:\n",
    "        sgnl_wnd = rollingWindow(sgnl[FIRST_WINDOW:], SAMPLE_WINDOW)\n",
    "        sgnl_picks = np.random.randint(0, sgnl_wnd.shape[0], [int(sgnl_wnd.shape[0] / SAMPLE_WINDOW * 0.5)])\n",
    "        if row['noisy']:\n",
    "            train_validate_noisy.append(sgnl_wnd[sgnl_picks])\n",
    "        else:\n",
    "            train_validate_clean.append(sgnl_wnd[sgnl_picks])\n",
    "for idx, row in sa.iterrows():\n",
    "    data, info = wfdb.rdsamp(row['fname'])\n",
    "    for sgnl in data.T:\n",
    "        sgnl_ds = signal.resample(sgnl, int(len(sgnl) / info['fs'] * MIN_FREQ))\n",
    "        sgnl_wnd = rollingWindow(sgnl_ds[FIRST_WINDOW:], SAMPLE_WINDOW)\n",
    "        sgnl_picks = np.random.randint(0, sgnl_wnd.shape[0], [int(sgnl_wnd.shape[0] / SAMPLE_WINDOW * 0.5)])\n",
    "        if row['noisy']:\n",
    "            train_validate_noisy.append(sgnl_wnd[sgnl_picks])\n",
    "        else:\n",
    "            train_validate_clean.append(sgnl_wnd[sgnl_picks])\n",
    "train_validate_noisy = np.concatenate(train_validate_noisy, 0)\n",
    "train_validate_clean = np.concatenate(train_validate_clean, 0)\n",
    "\n",
    "test_noisy = []\n",
    "test_clean = []\n",
    "for idx, row in val.iterrows():\n",
    "    data, info = wfdb.rdsamp(row['fname'])\n",
    "    for sgnl in data.T:\n",
    "        sgnl_wnd = rollingWindow(sgnl[FIRST_WINDOW:], SAMPLE_WINDOW)\n",
    "        sgnl_picks = np.random.randint(0, sgnl_wnd.shape[0], [int(sgnl_wnd.shape[0] / SAMPLE_WINDOW * 0.5)])\n",
    "        if row['noisy']:\n",
    "            test_noisy.append(sgnl_wnd[sgnl_picks])\n",
    "        else:\n",
    "            test_clean.append(sgnl_wnd[sgnl_picks])\n",
    "test_noisy = np.concatenate(test_noisy, 0)\n",
    "test_clean = np.concatenate(test_clean, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 3\n",
    "inner_dim = 32\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    batch_size = tf.shape(z_mean)[0]\n",
    "    epsilon = tf.random.normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + z_log_sigma * epsilon\n",
    "\n",
    "# Encoder\n",
    "input_x = tf.keras.layers.Input(shape= (SAMPLE_WINDOW, 1)) \n",
    "h = tf.keras.layers.LSTM(inner_dim, activation='relu')(input_x)\n",
    "\n",
    "# Z\n",
    "z_mean = tf.keras.layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = tf.keras.layers.Dense(latent_dim)(h)\n",
    "z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.keras.layers.RepeatVector(SAMPLE_WINDOW)(z)\n",
    "decoder = tf.keras.layers.LSTM(inner_dim, activation='relu', return_sequences=True)(decoder)\n",
    "decoder = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(decoder)\n",
    "\n",
    "def vae_loss(input_x, decoder, z_log_sigma, z_mean):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    recon = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(input_x, decoder))\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl = 0.5 * tf.reduce_mean(tf.exp(z_log_sigma) + tf.square(z_mean) - 1. - z_log_sigma)\n",
    "\n",
    "    return recon + kl\n",
    "\n",
    "model = tf.keras.Model(input_x, decoder)\n",
    "model.add_loss(vae_loss(input_x, decoder, z_log_sigma, z_mean))\n",
    "model.compile(loss=None, optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenClean = train_validate_clean.shape[0]\n",
    "\n",
    "trainX, valX = train_validate_clean[:int(0.85 * lenClean)], train_validate_clean[int(0.85 * lenClean):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "SHUFFLE_SIZE = BATCH_SIZE * 4\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(trainX[..., np.newaxis].astype(np.float32)).shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(valX[..., np.newaxis].astype(np.float32)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "109/109 [==============================] - 522s 5s/step - loss: 0.0953 - val_loss: inf\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 517s 5s/step - loss: 0.0820 - val_loss: inf\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 515s 5s/step - loss: 0.0738 - val_loss: inf\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 490s 4s/step - loss: 0.0675 - val_loss: inf\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 489s 4s/step - loss: 0.0652 - val_loss: inf\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 483s 4s/step - loss: 0.0625 - val_loss: inf\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 478s 4s/step - loss: 0.0608 - val_loss: inf\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 487s 4s/step - loss: 0.0599 - val_loss: inf\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 501s 5s/step - loss: 0.0586 - val_loss: inf\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 488s 4s/step - loss: 0.0566 - val_loss: inf\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - 120s 391ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(valX[..., np.newaxis].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07147962830593281"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.square(valX[..., np.newaxis] - preds).mean(1) > 0.1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 50s 393ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_noise = model.predict(train_validate_noisy[..., np.newaxis].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4482417038137692"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.square(train_validate_noisy[..., np.newaxis] - preds_noise).mean(1) > 0.1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results demonstrate that the VAE:\n",
    "1. classifies 7% of the subsignals in the clean signals as anomalous\n",
    "2. classifies 44% of the subsignals in the noisy signals as anomalous\n",
    "\n",
    "These results are comparable to the classifier in [ecgV1.ipynb](./ecgV1.ipynb), but I have not done much fine-tuning to this model yet.\n",
    "\n",
    "Besides that, it is much slower than our classifier. However, it looks promising given that it has only been trained on the clean signals, which could also contain noisy segments.\n",
    "\n",
    "A use of the VAE can be seen in [noisySegmentsVAE.ipynb](./noisySegmentsVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./mydata/vae/0010.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
